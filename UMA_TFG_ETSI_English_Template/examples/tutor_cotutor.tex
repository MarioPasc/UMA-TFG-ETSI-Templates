% ======================================================================
\documentclass{report}

%───────── Template, covers, back cover ────────────────────────────────
\usepackage{template/sty_files/template-TFGenglish-uma}

\usepackage{template/sty_files/cotutor_cover-blueuma}
\usepackage{template/sty_files/cotutor_cover-whiteuma}

\usepackage{template/sty_files/backcover-uma}

%───────── Auxiliary packages ──────────────────────────────────────────
\usepackage{blindtext}
\usepackage{mwe}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{siunitx}
\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{nomencl}
\makenomenclature
\usepackage[
  backend=biber,
  style=numeric,
  sorting=nyt
]{biblatex}
\addbibresource{references.bib}

% Listings config
\lstset{
  basicstyle=\ttfamily\small,
  numbers=left,
  numberstyle=\tiny,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  tabsize=2
}

%======================================================================
\begin{document}

\frontmatter
%────────────────────── Blue front cover (cotutor) ─────────────────────
\MakeBlueUMACover{
  degree     = {Grado en Ingeniería de la Salud},
  mencion    = {Mención en Bioinformática},
  titleES    = {Reconstrucción y Segmentación de Imágenes Biomédicas con Redes Neuronales Profundas},
  titleEN    = {Deep Neural Networks for Biomedical Image Reconstruction and Segmentation},
  author     = {John Doe},
  tutor      = {Dr.~John Doe},
  cotutor    = {Dra.~Jane Doe},
  dept       = {Departamento de Lenguajes y Ciencias de la Computación},
  cityDate   = {Málaga, \today},
  logoLeft   = {template/logos/NEG-uma-logo.png},
  logoRight  = {template/logos/NEG-etsi-logo.png}
}


%────────────────────── White front cover (cotutor) ────────────────────
\MakeWhiteUMACover{
  degree      = {Grado en Ingeniería de la Salud},
  mencion     = {Mención en Bioinformática},
  titleES     = {Reconstrucción y Segmentación de Imágenes Biomédicas con Redes Neuronales Profundas},
  titleEN     = {Deep Neural Networks for Biomedical Image Reconstruction and Segmentation},
  author      = {John Doe},
  tutor       = {Dr.~John Doe},
  cotutor     = {Dra.~Jane Doe},
  dept        = {Departamento de Lenguajes y Ciencias de la Computación},
  cityDate    = {MÁLAGA, \today},
  defenseDate = {septiembre de 2025},
  logoLeft    = {template/logos/POS-uma-logo.png},
  logoRight   = {template/logos/POS-etsi-logo.png}
}


%────────────────────── Abstracts ──────────────────────────────────────
\renewcommand{\abstractname}{Resumen}
\begin{abstract}
Describimos un \emph{pipeline} de \textbf{reconstrucción SR} de RM y
\textbf{segmentación de tumores} basado en redes neuronales profundas.
Integramos una U-Net 3D para segmentación y un modelo SR tipo EDSR para
reconstrucción, con pérdidas compuestas que combinan L1, SSIM y TV.
Evaluamos en volúmenes cerebrales con métricas PSNR, SSIM y Dice, y
comparamos frente a baselines clásicos de interpolación y \emph{Random
Forest}. El sistema reduce artefactos por \emph{undersampling} y mejora
la precisión de contornos clínicamente relevantes.
\medskip

\noindent\textbf{Palabras clave:} imagen biomédica, U-Net, super-resolución,
segmentación, PSNR, SSIM, Dice
\end{abstract}

\renewcommand{\abstractname}{Abstract}
\begin{abstract}
We present a deep-learning pipeline for \textbf{MRI super-resolution} and
\textbf{tumor segmentation}. A 3D U-Net segments lesions, while an EDSR-style
SR module reconstructs high-resolution volumes. We optimize a composite loss
(L1 + SSIM + TV) and report PSNR, SSIM, and Dice on brain MRI.
Compared to classical interpolation and Random Forest baselines, the model
reduces undersampling artifacts and sharpens clinically relevant boundaries.
\medskip

\noindent\textbf{Keywords:} biomedical imaging, U-Net, super-resolution,
segmentation, PSNR, SSIM, Dice
\end{abstract}

%──────────────── Acknowledgments ───────────────────────────────────────
\begin{umaacknowledgments}
Agradezco su función a los clínicos colaboradores, así como al grupo de Inteligencia Computacional y Análisis de Imagen de la Universidad de Málaga.
\end{umaacknowledgments}

%────────────────────── TOC and lists ──────────────────────────────────
\cleardoublepage
\tableofcontents
\cleardoublepage
\listoffigures
\cleardoublepage
\listoftables
\cleardoublepage

%────────────────────── Nomenclature ───────────────────────────────────
\nomenclature{$x \in \mathbb{R}^{H\times W\times D}$}{HR volume}
\nomenclature{$y \in \mathbb{R}^{h\times w\times d}$}{LR volume}
\nomenclature{$A$}{Forward operator (blur+downsample)}
\nomenclature{$\hat x$}{Network reconstruction}
\nomenclature{$\mathcal{L}$}{Training objective}
\nomenclature{$\operatorname{PSNR}$}{Peak Signal-to-Noise Ratio}
\nomenclature{$\operatorname{SSIM}$}{Structural Similarity}
\nomenclature{$\operatorname{Dice}$}{Sørensen–Dice coefficient}
\printnomenclature
\cleardoublepage

\mainmatter
%────────────────────── Chapter 1 — Introduction ───────────────────────
\chapter{Introduction}
Deep neural networks improved biomedical image reconstruction and analysis.
We target MRI super-resolution and tumor segmentation in a unified workflow
(\cref{fig:pipeline_overview}).

\begin{figure}[ht]
  \centering
  \includegraphics[width=.85\linewidth]{example-image-a}
  \caption{Pipeline: LR MRI \(\rightarrow\) SR reconstruction \(\rightarrow\)
  segmentation. Training uses multi-task losses and shared augmentations.}
  \label{fig:pipeline_overview}
\end{figure}

%────────────────────── Chapter 2 — Fundamentals ───────────────────────
\chapter{Fundamentals}
\section{Imaging model and objectives}
Assume \(y = A x + n\). The SR network \(f_\theta\) estimates
\(\hat x = f_\theta(y)\).
We use the composite objective
\begin{equation}
  \mathcal{L}_{\mathrm{SR}}
  = \lambda_1 \lVert \hat x - x \rVert_1
  + \lambda_s \left(1-\operatorname{SSIM}(\hat x, x)\right)
  + \lambda_{\mathrm{tv}} \operatorname{TV}(\hat x).
  \label{eq:sr_loss}
\end{equation}
For segmentation logits \(z = g_\phi(\hat x)\) and mask \(m\),
\begin{equation}
  \mathcal{L}_{\mathrm{seg}}
  = 1 - \operatorname{Dice}\!\left(\sigma(z), m\right)
    + \beta\, \operatorname{CE}\!\left(\sigma(z), m\right).
  \label{eq:seg_loss}
\end{equation}
Total loss:
\(\mathcal{L} = \mathcal{L}_{\mathrm{SR}} + \gamma\,\mathcal{L}_{\mathrm{seg}}\).

\section{Metrics}
\begin{align}
\operatorname{PSNR}(x,\hat x) &= 10\log_{10}\!\frac{L^2}{\operatorname{MSE}(x,\hat x)},\\
\operatorname{SSIM}(x,\hat x) &= \frac{(2\mu_x\mu_{\hat x}+C_1)(2\sigma_{x\hat x}+C_2)}
{(\mu_x^2+\mu_{\hat x}^2+C_1)(\sigma_x^2+\sigma_{\hat x}^2+C_2)},\\
\operatorname{Dice}(p,m) &= \frac{2\sum p m}{\sum p + \sum m}.
\end{align}

%────────────────────── Chapter 3 — Methods ────────────────────────────
\chapter{Methods}
\section{Network design}
SR module: residual blocks with sub-pixel upsampling.
Segmentation module: 3D U-Net with skip connections. \cite{lecun2015deep}

\begin{algorithm}
\caption{Joint SR+Seg training}
\label{alg:train}
\begin{algorithmic}[1]
\Procedure{Train}{$\mathcal{D}$, $\theta$, $\phi$}
  \For{epoch $=1..E$}
    \For{batch $(y,x,m)\sim \mathcal{D}$}
      \State $\hat x \gets f_\theta(y)$
      \State $z \gets g_\phi(\hat x)$; $p\gets\sigma(z)$
      \State $\mathcal{L}\gets \mathcal{L}_{\mathrm{SR}}(\hat x,x)+\gamma\,\mathcal{L}_{\mathrm{seg}}(p,m)$
      \State Update $(\theta,\phi)$ by Adam on $\nabla \mathcal{L}$
    \EndFor
  \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Reference implementation (PyTorch)}
\begin{lstlisting}[language=Python,caption={Minimal SR+U\_Net skeleton (PyTorch)},label={lst:pytorch}]
# Module: sr_seg.py
# Purpose: Joint SR reconstruction and segmentation for MRI volumes
# Notes: Typed, logged, dataclass configs, minimal structure

from dataclasses import dataclass
from typing import Tuple
import logging, torch, torch.nn as nn
import torch.nn.functional as F

logger = logging.getLogger("srseg")
logger.setLevel(logging.INFO)

@dataclass
class TrainConfig:
    lr: float = 2e-4
    gamma: float = 1.0
    lambda_l1: float = 1.0
    lambda_ssim: float = 0.2
    lambda_tv: float = 1e-5

class ResidualBlock(nn.Module):
    """Residual block with two conv3d layers."""
    def __init__(self, c: int):
        super().__init__()
        self.c1 = nn.Conv3d(c, c, 3, padding=1)
        self.c2 = nn.Conv3d(c, c, 3, padding=1)
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        r = F.relu(self.c1(x))
        r = self.c2(r)
        return F.relu(x + r)

class SRNet(nn.Module):
    """EDSR-like SR backbone for volumetric data."""
    def __init__(self, in_ch: int = 1, base: int = 64, depth: int = 8):
        super().__init__()
        self.head = nn.Conv3d(in_ch, base, 3, padding=1)
        self.body = nn.Sequential(*[ResidualBlock(base) for _ in range(depth)])
        self.tail = nn.Conv3d(base, 1, 3, padding=1)
    def forward(self, y: torch.Tensor) -> torch.Tensor:
        h = self.head(y)
        h = self.body(h)
        return self.tail(h)

class UNet3D(nn.Module):
    """3D U-Net for segmentation."""
    def __init__(self, in_ch: int = 1, n_classes: int = 2, base: int = 32):
        super().__init__()
        self.e1 = nn.Sequential(nn.Conv3d(in_ch, base, 3, padding=1), nn.ReLU(),
                                nn.Conv3d(base, base, 3, padding=1), nn.ReLU())
        self.e2 = nn.Sequential(nn.Conv3d(base, base*2, 3, stride=2, padding=1), nn.ReLU())
        self.b  = nn.Sequential(nn.Conv3d(base*2, base*4, 3, padding=1), nn.ReLU())
        self.d2 = nn.Sequential(nn.ConvTranspose3d(base*4, base*2, 2, stride=2), nn.ReLU())
        self.out= nn.Conv3d(base*2, n_classes, 1)
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x1 = self.e1(x)
        x2 = self.e2(x1)
        b  = self.b(x2)
        d2 = self.d2(b)
        z  = self.out(d2)
        return z

def dice_loss(p: torch.Tensor, m: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
    p = p.softmax(1)[:,1]  # foreground
    m = (m > 0.5).float()
    num = 2*(p*m).sum()
    den = p.sum() + m.sum() + eps
    return 1 - num/den

def tv(x: torch.Tensor) -> torch.Tensor:
    return (x[:, :, 1:] - x[:, :, :-1]).abs().mean() + \
           (x[:, :, :, 1:] - x[:, :, :, :-1]).abs().mean() + \
           (x[:, :, :, :, 1:] - x[:, :, :, :, :-1]).abs().mean()

def train_step(sr: SRNet, unet: UNet3D, opt: torch.optim.Optimizer,
               y: torch.Tensor, x: torch.Tensor, m: torch.Tensor,
               cfg: TrainConfig) -> float:
    opt.zero_grad()
    x_hat = sr(y)
    l1 = (x_hat - x).abs().mean()
    # crude SSIM proxy to keep example short
    ssim_term = ((x_hat - x)**2).mean()
    l_tv = tv(x_hat)
    z = unet(x_hat.detach())
    ce = F.cross_entropy(z, m.long())
    dl = dice_loss(z, m)
    loss = cfg.lambda_l1*l1 + cfg.lambda_ssim*ssim_term + cfg.lambda_tv*l_tv + cfg.gamma*(ce + dl)
    loss.backward()
    opt.step()
    logger.info(f"loss={loss.item():.4f}")
    return float(loss.item())
\end{lstlisting}

%────────────────────── Chapter 4 — Results ────────────────────────────
\chapter{Results}
\section{Quantitative metrics}
\begin{table}[ht]
  \centering
  \caption{SR and segmentation performance (mean \(\pm\) std).}
  \label{tab:metrics}
  \begin{tabular}{l S[table-format=2.2] S[table-format=2.2] S[table-format=1.3]}
    \toprule
    Method & {PSNR [dB]} & {SSIM} & {Dice} \\
    \midrule
    Bicubic + RF     & 28.31 & 0.842 & 0.786 \\
    EDSR + U\textsuperscript{3}Net & 31.75 & 0.904 & 0.865 \\
    Proposed (joint) & 32.18 & 0.918 & 0.882 \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Qualitative examples}
\begin{figure}[ht]
  \centering
  \includegraphics[width=.9\linewidth]{example-image-b}
  \caption{Axial slices: LR input, SR output, and segmentation overlay.}
  \label{fig:qualitative}
\end{figure}

%────────────────────── Chapter 5 — Discussion ─────────────────────────
\chapter{Discussion}
Joint optimization improves boundary fidelity and suppresses ringing.
Failure cases include motion artifacts and unseen acquisition protocols.

%────────────────────── Appendices ─────────────────────────────────────
\begin{umaappendices}

\umaappendix{Data splits and preprocessing}
\section{Preprocessing}
Bias-field correction, z-score normalization per volume, random flips and
intensity jitter for augmentation.

\umaappendix{Additional code snippets}

\section{Evaluation script (Python)}
\begin{lstlisting}[language=Python,caption={Metric computation script},label={lst:metrics}]
# eval_metrics.py
# Computes PSNR, SSIM (skimage), and Dice given predictions and references.

from skimage.metrics import structural_similarity as ssim
import numpy as np

def psnr(x, y, peak=1.0):
    mse = np.mean((x - y) ** 2)
    return 10 * np.log10(peak**2 / (mse + 1e-12))

def dice(p, m, thr=0.5):
    pbin = (p >= thr).astype(np.float32)
    m = m.astype(np.float32)
    return 2*(pbin*m).sum() / (pbin.sum() + m.sum() + 1e-12)
\end{lstlisting}

\section{R quick-look plot}
\begin{lstlisting}[language=R,caption={Boxplots of PSNR and Dice},label={lst:rplots}]
# rplots.R
library(ggplot2)
df <- data.frame(
  method = rep(c("Bicubic+RF","EDSR+U3Net","Joint"), each=10),
  psnr = c(rnorm(10,28.3,0.6), rnorm(10,31.8,0.5), rnorm(10,32.2,0.4)),
  dice = c(rnorm(10,0.786,0.02), rnorm(10,0.865,0.02), rnorm(10,0.882,0.02))
)
ggplot(df, aes(method, psnr)) + geom_boxplot() + theme_minimal()
ggplot(df, aes(method, dice)) + geom_boxplot() + theme_minimal()
\end{lstlisting}

\end{umaappendices}

%────────────────────── Back cover and bibliography ────────────────────
\backmatter
\printbibliography[title={Bibliografía}]
\cleardoublepage
\MakeUMABackCover

\end{document}
% ======================================================================
